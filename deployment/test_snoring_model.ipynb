{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pO4-CY_TCZZS"
   },
   "source": [
    "# Train a Simple Audio Recognition Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BaFfr7DHRmGF"
   },
   "source": [
    "This notebook demonstrates how to train a 20 kB [Simple Audio Recognition](https://www.tensorflow.org/tutorials/sequences/audio_recognition) model to recognize keywords in speech.\n",
    "\n",
    "The model created in this notebook is used in the [micro_speech](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech) example for [TensorFlow Lite for MicroControllers](https://www.tensorflow.org/lite/microcontrollers/overview).\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/train/train_micro_speech_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/train/train_micro_speech_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XaVtYN4nlCft"
   },
   "source": [
    "**Training is much faster using GPU acceleration.** Before you proceed, ensure you are using a GPU runtime by going to **Runtime -> Change runtime type** and set **Hardware accelerator: GPU**. Training 15,000 iterations will take 1.5 - 2 hours on a GPU runtime.\n",
    "\n",
    "## Configure Defaults\n",
    "\n",
    "**MODIFY** the following constants for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ludfxbNIaegy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training these words: snoring,no_snoring\n",
      "Training steps in each stage: 25,25\n",
      "Learning rate in each stage: 0.005,0.005\n",
      "Total number of training steps: 50\n"
     ]
    }
   ],
   "source": [
    "# A comma-delimited list of the words you want to train for.\n",
    "# The options are: yes,no,up,down,left,right,on,off,stop,go\n",
    "# All the other words will be used to train an \"unknown\" label and silent\n",
    "# audio data with no spoken words will be used to train a \"silence\" label.\n",
    "WANTED_WORDS = \"snoring,no_snoring\"\n",
    "\n",
    "# The number of steps and learning rates can be specified as comma-separated\n",
    "# lists to define the rate at each stage. For example,\n",
    "# TRAINING_STEPS=12000,3000 and LEARNING_RATE=0.001,0.0001\n",
    "# will run 12,000 training loops in total, with a rate of 0.001 for the first\n",
    "# 8,000, and 0.0001 for the final 3,000.\n",
    "TRAINING_STEPS = \"25,25\"\n",
    "LEARNING_RATE = \"0.005,0.005\"\n",
    "WINDOW_STRIDE = 20\n",
    "PREPROCESS = \"micro\"\n",
    "DATASET_DIR = '/home/jinhao/Snoring-Detection/Snoring_Dataset_@16000/'\n",
    "# Calculate the total number of steps, which is used to identify the checkpoint\n",
    "# file name.\n",
    "TOTAL_STEPS = str(sum(map(lambda string: int(string), TRAINING_STEPS.split(\",\"))))\n",
    "LOGS_DIR = 'logs/'\n",
    "# Print the configuration to confirm it\n",
    "print(\"Training these words: %s\" % WANTED_WORDS)\n",
    "print(\"Training steps in each stage: %s\" % TRAINING_STEPS)\n",
    "print(\"Learning rate in each stage: %s\" % LEARNING_RATE)\n",
    "print(\"Total number of training steps: %s\" % TOTAL_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 16:29:21.719929: E tensorflow/core/platform/hadoop/hadoop_file_system.cc:132] HadoopFileSystem load error: libhdfs.so: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 16:29:40.089197: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 6091720 exceeds 10% of system memory.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../deployment/tensorflow1/tensorflow/examples/speech_commands/\")\n",
    "import input_data\n",
    "import models\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "CLIP_DURATION_MS = 1000\n",
    "WINDOW_SIZE_MS = 30.0\n",
    "FEATURE_BIN_COUNT = 40\n",
    "BACKGROUND_FREQUENCY = 0\n",
    "BACKGROUND_VOLUME_RANGE = 0\n",
    "TIME_SHIFT_MS = 0.0\n",
    "SILENCE = 10\n",
    "DATA_URL = '' #'https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
    "VALIDATION_PERCENTAGE = 10\n",
    "TESTING_PERCENTAGE = 10\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "model_settings = models.prepare_model_settings(\n",
    "    len(input_data.prepare_words_list(WANTED_WORDS.split(','))),\n",
    "    SAMPLE_RATE, CLIP_DURATION_MS, WINDOW_SIZE_MS,\n",
    "    WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS)\n",
    "audio_processor = input_data.AudioProcessor(\n",
    "    DATA_URL, DATASET_DIR,SILENCE,10,\n",
    "    WANTED_WORDS.split(','), VALIDATION_PERCENTAGE,\n",
    "    TESTING_PERCENTAGE, model_settings, LOGS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "# Check version of python using\n",
    "# https://medium.com/@nrk25693/how-to-add-your-conda-environment-to-your-jupyter-notebook-in-just-4-steps-abeab8b8d084\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io.wavfile import read as wav_read\n",
    "import io,os\n",
    "import ffmpeg\n",
    "#import librosa\n",
    "import scipy.io.wavfile\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run inference (on a single input this time)\n",
    "# Note: this also includes additional manual pre-processing\n",
    "def run_tflite_inference_singleFile(tflite_model_path, custom_audio, sr_custom_audio, model_type=\"Float\"):\n",
    "  #\n",
    "  # Preprocess the sample to get the features we pass to the model\n",
    "  #\n",
    "  # First re-sample to the needed rate\n",
    "#   custom_audio_resampled = librosa.resample(np.float64(custom_audio), sr_custom_audio, SAMPLE_RATE)\n",
    "#   # Then extract the loudest one second\n",
    "#   scipy.io.wavfile.write('custom_audio.wav', SAMPLE_RATE, np.int16(custom_audio_resampled))\n",
    "#   os.system('./extract_loudest_section/gen/bin/extract_loudest_section custom_audio.wav ./trimmed')\n",
    "#   # Finally pass it through the TFLiteMicro preprocessor to produce the \n",
    "#   # spectrogram/MFCC input that the model expects\n",
    "#   custom_model_settings = models.prepare_model_settings(\n",
    "#       0, SAMPLE_RATE, CLIP_DURATION_MS, WINDOW_SIZE_MS,\n",
    "#       WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS)\n",
    "\n",
    "  custom_audio_processor = input_data.AudioProcessor(None, None, 0, 0,'', 0, 0,\n",
    "                                                    model_settings, None)\n",
    "  custom_audio_preprocessed = custom_audio_processor.get_features_for_wav(\n",
    "                                        custom_audio, model_settings, TF_SESS)\n",
    "  # Reshape the output into a 1,1960 matrix as that is what the model expects\n",
    "  custom_audio_input = custom_audio_preprocessed[0].flatten()\n",
    "#   print (custom_audio_input)\n",
    "#   custom_audio_input = custom_audio\n",
    "#   print (custom_audio_input)\n",
    "\n",
    "  test_data = np.reshape(custom_audio_input,(1,len(custom_audio_input)))\n",
    "\n",
    "  #\n",
    "  # Initialize the interpreter\n",
    "  #\n",
    "  interpreter = tf.lite.Interpreter(tflite_model_path)\n",
    "  interpreter.allocate_tensors()\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "  #\n",
    "  # For quantized models, manually quantize the input data from float to integer\n",
    "  #\n",
    "  if model_type == \"Quantized\":\n",
    "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "    test_data = test_data / input_scale + input_zero_point\n",
    "    test_data = test_data.astype(input_details[\"dtype\"])\n",
    "\n",
    "  #\n",
    "  # Run the interpreter\n",
    "  #\n",
    "  interpreter.set_tensor(input_details[\"index\"], test_data)\n",
    "  interpreter.invoke()\n",
    "  output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "  top_prediction = output.argmax()\n",
    "\n",
    "  #\n",
    "  # Translate the output\n",
    "  #\n",
    "  top_prediction_str = ''\n",
    "  if top_prediction == 0 or top_prediction == 1:\n",
    "    top_prediction_str = WANTED_WORDS.split(',')[top_prediction]\n",
    "  elif top_prediction == 2:\n",
    "    top_prediction_str = 'silence'\n",
    "  else:\n",
    "    raiseError('Label not defined')\n",
    "\n",
    "  print('%s model guessed the value to be %s' % (model_type, top_prediction_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing snoring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinhao/.local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py:1748: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'summaries_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12143/3665126023.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Then test the model -- do they all work as you'd expect?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing snoring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrun_tflite_inference_singleFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv_micro/conv_model.tflite'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDATASET_DIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'snoring/1_44.wav'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m16000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Quantized\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing no snoring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrun_tflite_inference_singleFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv_micro/conv_model.tflite'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDATASET_DIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'no_snoring/0_14.wav'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m16000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Quantized\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12143/182827223.py\u001b[0m in \u001b[0;36mrun_tflite_inference_singleFile\u001b[0;34m(tflite_model_path, custom_audio, sr_custom_audio, model_type)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   custom_audio_processor = input_data.AudioProcessor(None, None, 0, '', 0, 0,\n\u001b[0;32m---> 19\u001b[0;31m                                                     model_settings, None)\n\u001b[0m\u001b[1;32m     20\u001b[0m   custom_audio_preprocessed = custom_audio_processor.get_features_for_wav(\n\u001b[1;32m     21\u001b[0m                                         custom_audio, model_settings, TF_SESS)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'summaries_dir'"
     ]
    }
   ],
   "source": [
    "TF_SESS = tf.compat.v1.InteractiveSession()\n",
    "# Then test the model -- do they all work as you'd expect?\n",
    "print(\"Testing snoring\")\n",
    "run_tflite_inference_singleFile('conv_micro/conv_model.tflite',DATASET_DIR+'snoring/1_44.wav' , 16000, model_type=\"Quantized\")\n",
    "print(\"Testing no snoring\")\n",
    "run_tflite_inference_singleFile('conv_micro/conv_model.tflite',DATASET_DIR+'no_snoring/0_14.wav' , 16000, model_type=\"Quantized\")\n",
    "TF_SESS.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_micro_speech_model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
